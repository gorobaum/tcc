\section{Conceitos e Técnologias}
\subsection{High-Performance Computability}
HPC nasceu da necessidade de poder computacional para resolver uma série de problemas computacionalmente caros, entre eles:
\begin{itemize}
  \item Previsão climática
  \item Modelação molecular
  \item Simulações físicas
  \item Física quântica
\end{itemize}
Até o final dos anos 90 todos os supercomputadores tinham como base processadores vetoriais. Só no final da década seguinte, 
com o aumento do desempenho das GPUs, alguns supercomputadores começaram a usar GPUs como elemento de processamento.
\subsection{GPU}
A primeira GPU foi a GeForce 256, da NVIDIA, lançada em 1999. O hardware seguia um pipeline de 2 fases, uma que aplicava transformações
em vértices e outro em pixels. Em 2001, a GeForce 3 trouxe o primeiro processador de vértices programável. Em 2005 a primeira
GPU com um processador unificado, usado tanto para operações em pixels como em vértices, foi lançada para o console XBox 360. 
Para unificar os 2 processos do pipeline num único processador foi necessário generalizar esse processador, e essa generalização
abriu as portas para programas genericos executarem na GPU.

A placa usada para os testes desse trabalho, a GeForce GTX460, usa a arquitetura Fermi, a segunda mais nova arquitetura da NVIDIA para
GPUs. Essa arquitetura separa o fluxo de execução baseando-se no tipo de aplicação que será executada nela. Existe um fluxo para
aplicações gráficas e outro para aplicações genéricas, que é o foco desse trabalho.

A placa contém um escalonador para threads implementado em hardware. Ele é responsável por escalonar as threads que serão
executadas nos streaming multiprocessors (SM). Um SM é um conjunto de 48 processadores, um pequeno bloco de memória própria,
um cache de instruções e 8 unidades de funções gráficas. A Geforce GTX 460 tem 7 SMs, totalizando 336 processadores.

O código que será executado em cada processador é chamado de \textbf{kernel}. Ao executar um kernel na GPU, o 
hardware criará threads, cada uma delas executando o mesmo código, mas com dados diferentes. Nas placas NVIDIA as threads 
são agrupadas em blocos, e esses blocos são escalonados para cada SM. Depois, todas as threads dentro de um bloco são 
divididas em pequenos grupos chamados de \textbf{warp}\cite{paulo}, e cada warp é executado paralelamente dentro do 
mesmo SM para qual o bloco foi escalonado. Existe um limite para a quantidade de threads escalonadas para execução
dentro de um SM, que é definida pelos recursos que cada thread consome. Por exemplo, não há como executar 10 threads
que consomem 10 registradores cada em um SM com 90 registradores.

Outra parte importante do hardware é a memória, que é limitada em relação à da CPU. GPUs tem, em média, 1GB
de memória, enquanto CPUs tem 4GB. O acesso a um mesmo bloco de memória é concorrente, mas ao utilizar caches e leitura ou escritas em
conjunto podemos minimizar a taxa com que leituras ou escritas conflitantes são feitas. Mas ainda sim é necessário atenção ao escrever um
kernel. Dada a estrutura do hardware da GPU, é melhor deixar threads que façam operações sobre posições de memória próximas no mesmo
SM, assim elas podem utilizar a memória compartilhada do mesmo, e elas podem requisitar em conjunto um mesmo bloco da memória principal,
se necessário.

No caso da GTX460 cada SM tem um bloco de memória de 64KB. Esse bloco pode ser configurado para 16KB de memória compartilhada e 48KB
de cache L1 ou vice versa. A memória principal da placa é de 1024MB com conexões de 256 bits. A placa também tem um
cache L2 de 512KB.

Outro fator limitante é a transferência de dados da memória principal do computador para a memória 
principal da GPU. A transmissão é feita por um barramento PCI Express, com velocidades de até 16GB/s ( dado que o
barramento seja utilizado somente pela GPU ). Essa transmissão é a parte mais lenta de todo o
processo de execução na GPU e dado isso, em alguns casos é mais viável executar na GPU um pedaço do seu programa que seria executado
na CPU do que retornar os dados computados na GPU para a CPU, executar esse pedaço especifico, e passá-los de volta para a GPU 
para mais operações e novamente retornar esses dados para a CPU no final, passando duas vezes a mais pelo PCI Express. 

Ao estudar como o código é executado nas GPUs NVIDIA descobrimos a existência de uma máquina virtual chamada de Parallel Thread Execution\cite{ptx}.
Todo kernel é primeiro compilado para um arquivo .ptx que é executado na GPU através da máquina PTX. Ela é utilizada para garantir 
a retrocompatibilidade de kernels em placas mais antigas.

\subsection{CUDA}
\textit{Compute Unified Device Architecture} (CUDA)\cite{cuda} é uma arquitetura de programação para GPUs criada pela NVIDIA.
Ele adiciona suas diretrizes para as linguagens C, C++, FORTRAN e Java, permitindo que elas usem a GPU.
Esse trabalho usa o CUDA junto com a linguagem C.
A versão 1.0 do CUDA foi disponibilizada no inicio de 2007. Atualmente só existe um compilador para CUDA, o nvcc,
e ele só da suporte para GPUs NVIDIA.

Para uma função executar na GPU ela precisa ser invocada de um programa da CPU. Chamamos esse programa de \textit{Host}
e a GPU onde o kernel irá executar de \textit{Device}.

O CUDA implementa um conjunto virtual de instruções e memória, tornando os programas retroativos. O compilador
primeiro compila o código em C para um intermediário, chamado de PTX, que depois será convertido em linguagem
de máquina. Na conversão do PTX para linguagem de máquina o compilador verifica quais instruções o \textit{device}
suporta e converte o código para usar as instruções corretas.
Para obter o maior desempenho possível, é importante saber para qual versão o código final será compilado, 
pois na passagem do código de uma versão maior para uma menor não existe a garantia que o algoritmo seguira as mesmas instruções, 
o compilador pode mudar um conjunto de instruções para outro menos eficiênte, ou em alguns casos, algumas instruções não existem em
versões mais antigas do hardware.

\subsubsection{Modelo de Plataforma}
A inicialização dos recursos que o CUDA necessita para a comunicação com a GPU é feita no background da
aplicação no momento da primeira chamada de alguma das diretivas do CUDA. Essa primeira diretiva terá um
tempo maior de execução que chamadas subsequentes a mesma diretiva. Na inicialização o CUDA identifica
os \textit{devices} existentes e escolhe um deles para ser o responsável pelas execuções posteriores.

O próximo passo é a alocação de memória no \textit{device}. As operações de leitura de memória de um kernel são feitas somente
na memória de um \textit{device}. A alocação dessa memória é feita pelo \textit{host}, usando \verb#cudaMalloc()#. 
Para copiar a memória do \textit{host} para o \textit{device} ou vice-versa,
\verb#cudaMemcpy()# é usada. Para liberar o espaço alocado após a execução basta usar o \verb#cudaFree()#.
Todas essas diretivas recebem um ponteiro do \textit{host}, usado para o controle sobre qual posição da memória está sendo
operado em cada operação.

O CUDA dá suporte a alocação de vetores em duas ou três dimensões através de: \verb#cudaMallocPitch()# e 
\verb#cudaMalloc3D()#, respectivamente. É necessário usar as modificações dos comandos \verb#Memcpy# para
duas ou três dimensões também, que são: \verb#cudaMemcpy2D()#, \verb#cudaMemcpy3D()#.

\subsubsection{Modelo de Programação}
Um kernel no CUDA é uma função C que será executada paralelamente $n$ vezes em $n$ threads diferentes na GPU. Um kernel pode ser
definido em qualquer lugar do seu código, usando a declaração \verb#__global__# do lado esquerdo do tipo de retorno do kernel.
Para invocar um kernel, o \textit{host} faz a chamada de uma função com a sintaxe parecida com o C, mas usa uma configuração de
execução definida pelo CUDA, que usa a sintaxe \verb#<<<...>>># junto da chamada da função. Os parâmetros da configuração são
o número de blocos de threads e o número de threads por blocos. Para somar dois vetores de tamanho M e guardar o resultado num
outro vetor, o código é o seguinte:

\begin{verbatim}
  __global__ void MatrixMulti ( float* a, float* b, float* c) { 
    int i = threadIdx.x;
    a[i] = b[i] + c[i];        
  }
                            
  int main () {               
    ...                       
    VecAdd<<<1,M>>>(a, b, c)  
    ...                       
  }                                 
\end{verbatim}

No kernel acima, a linha \verb#int i = threadIdx.x# atribui a variável i o valor do indice da thread atual na primeira dimensão. 
A estrutura \verb#threadIdx# é um vetor de 3 dimensões, logo as threads podem ser organizadas em 1, 2 ou 3 dimensões dentro de um
\textit{device}. As threads são organizadas por blocos. Cada bloco tem dimensões maleáveis, mas as GPUs atuais limitam para 1024 o 
número máximo de threads por blocos. Cada bloco é lançado para execução em um processador diferente. Blocos são organizados em 
grids, que tem seu tamanho configurado na chamada o kernel, bem como o tamanho de cada bloco. No nosso exemplo acima, na linha
\verb#VecAdd<<<1,M>>>(a,b,c)#, o 1 determina o número de blocos e o M o número de threads por bloco.

O CUDA supõem que todos os blocos podem ser executados de maneira independende, ou seja, eles podem executar tanto paralelamente
quanto sequencialmente. Com isso, é possivel que o desempenho do código aumente em GPUs com mais processadores, sem que o programador
tenha que modificar o código.

O CUDA sabe qual instruções ele pode executar dentro de um \textit{device} baseando-se no seu Compute Capability 
(Capacidade Computacional). A Compute Capability de um \textit{device} são dois números, um que representa a arquitetura do 
\textit{device}, e outro que representa melhorias numa arquitetura.
A arquitetura \textit{Tesla}, a primeira da NVIDIA a dar suporte a GPGPU, tem Compute Capability 1.x, a seguinte, a \textit{Tesla},
tem 2.x e a atual, a \textit{Kepler}, tem 3.x. Dentro de cada arquitetura, podem existir melhorias nas instruções, que são
refletidas no número após o ponto, ou seja, uma placa com Compute Capability 2.1 tem instruções que uma 2.0 não tem.

\subsubsection{Hierarquia de Memória}
No CUDA, a memoria é separada lógicamente em 4 locais:

\begin{itemize}
  \item Registradores - Toda variável de uma thread fica em registradores.
  \item Memória Local - Memória acessivel por cada thread separadamente, mas de uso pouco provável. Ela só é usada se
          não existe mais espaço nos registradores ou se o compilador não ter certeza sobre o tamanho de um vetor.
  \item Memória Compartilhada - Cada bloco de threads tem uma memória compratilhada. A memória compartilhada é separada em
          pequenos blocos independentes. Se uma requisição de leitura tem n endereços em n blocos diferentes, o tempo de leitura
          desses n endereços é igual ao tempo de leitura de 1 endereço. Caso duas leituras caiam no mesmo bloco, elas serão
          serializadas. A memória compatilhada fica em chips dentro dos SMs, logo seu acesso é mais rápido do que o acesso a
          memória global.
  \item Memória Global - A memória global é acessivel por qualquer bloco em execução em um \textit{device}. A memoria global não é
          resetada após a execução de um kernel, então chamadas subsequentes de um mesmo kernel simplesmente leêm os resultados
          da memória global. Existe um pedaço da memória global reservada para valores constantes do programa.
\end{itemize}

Por padrão, o compilador do CUDA cuida do gerenciamento da memória, ou seja, ele é o responsável por distribuir os dados 
entre os locais diferentes de memória. O programador pode dar dicas para o compilador usando qualificadores indicando o local
que ele quer que aquele elemento fique na memória. Os possiveis qualificadores são:
\begin{itemize}
  \item \verb#__device__# Fica na memória global.
  \item \verb#__constant__#   Fica na area constante da memória global.
  \item \verb#__shared__# Fica na memória compartilhada das threads.
  \item \verb#__restrict__# Indica para o compilador que todos os ponteiros com esse qualificador apontam para locais diferentes
                            da memória. Isso é importante pois o compilador pode fazer otimizações com o código sabendo dessa informação.   
\end{itemize}

GPUs com Compute Cabapility maior ou igual a 2.0 podem alocar memória dentro do \textit{device} em tempo de execução.

\subsection{OpenCL}
Open Computing Language (OpenCL)\cite{khronosopencl} é uma framework aberta para programação genérica para varios procesadores, 
dentre eles GPUs e CPUs.
OpenCL da suporte para sistemas embarcados, sistemas pessoais, corporativos e até HPC. Ele consegue isso criando uma interface
de baixo nível, ou seja, o mais próximo do hardware possivel, e mantendo auto desempenho, com uma abstração protátil. O OpencL
também é uma API para controle de aplicações paralelas em sistemas com processadores heterogéneos. O OpenCL consegue, numa mesma
aplicação, reconhecer vários processadores diferentes dentro de um mesmo computador, e executar códigos distintos entre eles,
coordenando os hardwares. Aqui, como no CUDA, a parte do código executado na CPU é chamada de \textit{Host} e o hardware
que executa os kernels de \textit{Devices}. É importante lembrar que dado essa generalização do OpenCL, é possível que a CPU
onde o código do \textit{host} esteja executando seja usada para rodar um kernel, e essa CPU passa a ser um \textit{device}
ao mesmo tempo em que roda o \textit{host}. Tanto o fato do OpenCL ser aberto quanto o fato dele não se restringir a um
hardware especifico fazem dele a linguagem mais usada para GPGPU fora de GPUs NVIDIA.

O framework do OpenCL pode ser explicado usando 4 modelos hierarquicos, que são:
\begin{itemize}
  \item Plataforma
  \item Memória
  \item Execução
  \item Programação
\end{itemize}

\subsubsection{Modelo de Plataforma}
No OpenCL existe um \textit{host} conectado a um ou mais \textit{devices}. Os \textit{devices} são abstrações de uma GPU ou de uma CPU.
Cada \textit{device} é composto de uma ou mais Compute Unit (CU), e cada CU é composto de um ou mais Processing Element (PE). 
Por exemplo, uma CPU com 2 cores seria vista pelo OpenCL como um \textit{device} com uma Compute Unit e 2 PRocessing Elements. 
O processamento dentro de um \textit{device} ocorre num PE. O processamento é iniciado atravez de \textit{comandos} que o \textit{host} 
manda para o \textit{device}. Os PEs podem executar tanto no modelo de SIMD (Instrução Única, Múltiplos Dados) ou SPMD 
(Processo Único, Múltiplos Dados). No SIMD, todas as threads executam a mesma operação ao mesmo tempo em dados diferentes
e no SPMD cada thread tem um ponteiro de instrução próprio. O responsável por iniciar a execução dos kernels nos PE é o \textit{host}.

O OpenCL tem suporte para varios tipos de \textit{devices} diferentes: GPUs, CPUs, DSP ou Cell/B.E. .
Para manter a retrocompatibilidade do código, cada device guarda 3 números importantes para o OpenCL:
\begin{itemize} 
  \item A versão da plataforma - Indica qual a versão da API que o \textit{host} pode usar para se comunicar com o OpenCL.
        Diz respeito ao contexto, objetos de memória, filas de comando e \textit{devices}.
  \item A versão do \textit{device} - Indica qual a capacidade de um \textit{device}, como possíveis funções implementadas
        em hardware ou limites de memória.
  \item A versão da linguagem - Indica o número de features do OpenCL implementadas no \textit{device}.
\end{itemize}

O \textit{host} usa a versão da linguagem para determinar o que pode ou não ser feito no \textit{device} em momento de compilação.
A versão da linguagem nunca é maior que a versão da plataforma, mas pode ser maior que a versão do \textit{device}.

\subsubsection{Modelo de Execução}

Com as plataformas definidas, vamos entender como o OpenCL cuida da execução dos kernels dentro de uma plataforma.
Cada instância de um kernel rodando dentro de um Processing Element é chamada de Work-Item. Dentro de um \textit{device}
é criado um conjunto de indices de até 3 dimensões, onde cada ponto dentro desse conjunto de indices é um work-item. 
Como visto acima, cada work-item executa o mesmo código, mas com dados diferentes e, existinto pulos condicionais no código, 
o caminho de execução pode váriar.

Esse conjunto de indices é chamado de NDRange. Ele é definido por um vetor de tamanho N, N sendo o número de dimensões do
NDRange, em que cada componente do vetor determina o tamanho de cada dimensão do NDRange. 

Os work-items estão organizados dentro de work-groups. O OpenCL escalona a execução dos work-groups, ou seja, ele envia um 
work-group para a execução, fazendo com que todos os work-items dentro dele sejam executados, e quando esse terminar sua 
execução um novo work-group com novos work-items é enviado para execução até que todos os work-items sejam executados. 
O número de dimensões do NDRange, de work-items por dimensão do NDRange e o número de work-items por dimensão de um work-group
devem ser definidos pelo \textit{host} antes da chamada de execução do kernel. O número de work-items é definido pela multiplicação
o número de work-items por dimensão do NDRange, e a quantidade de work-items por work-groups é definida pela multiplicação das dimensões
de um work-group.

Cada work-item é identificado através de um ID único global ou um ID único local dentro de um work-group. Cada work-group é identificado
por um ID global único, logo um work-item pode ser identificado ou pelo seu ID global ou pela combinação do seu ID local e do ID do seu 
work-group. Esses IDs são tuplas de 1, 2 ou 3 indices, variando de acordo com o tamanho do NDRange. Os indices desses IDs vão de $M$ até
$M+\delta$, $\delta$ sendo o tamanho da dimensão que a tupla representa e $M$ o um valor inicial para os indices daquela dimensão definido
na criação do NDRange pelo \textit{host}.

Para controlar a execução de vários kernels ao mesmo tempo em \textit{devices} diferentes, o OpenCL define um \textbf{Context}.
Um \textit{Context} é um conjunto de \textit{Devices}, \textit{Kernels}, \textit{Program Objects} e \textit{Memory Objects}.
\textit{Devices} e \textit{Kernels} já foram explicados acima, e \textit{Memory Objects} serão explicados na subseção abaixo.
\textit{Program Objects} são objetos que tem as seguintes informações:
\begin{itemize}
  \item Binário que será transformado nas funções de um ou mais kernels;
  \item O número de kernels dentro desse binário;
  \item O log da compilação, caso necessário;
  \item Uma referência para o \textit{context} e os \textit{devices} que ele está associado.
\end{itemize}

O binário de um \textit{Program Object} pode ser compilado em tempo de execução por uma função do OpenCL.

Com um \textit{conext} criado e inicializado, o \textit{host} controla a execução dele usando um objeto chamado \textbf{Command-Queue}.
O \textit{host} adiciona comando a uma \textit{command-queue} que está associada a um \textit{context}, e os comandos são executados
dentro dos \textit{devices} do \textit{context}. Os comandos são divididos em 3 tipos:
\begin{itemize}
  \item Comandos de execução de kernel;
        \begin{lstlisting}
          clEnqueueNDRangeKernel(queue, kernel, 2, NULL, 
              work_dim, local_dim, 0, NULL, &event);
        \end{lstlisting}
  \item Comandos de transferência de memória;
        \begin{lstlisting}
          clEnqueueWriteBuffer(queue, columnSize, CL_TRUE, 
              0, sizeof(int), &sizeC, 0, NULL, &event);
        \end{lstlisting}
  \item Comandos de sincronização.
        \begin{lstlisting}
          clFinish(queue);
        \end{lstlisting}
\end{itemize}
Esses comandos podem ser executados sequêncialmente, onde um comando na \textit{command-queue} espera todos os anteriores a ele executarem para
executar, ou de forma não sequêncial, onde a \textit{command-queue} só define a ordem em que os comandos terão sua execução iniciada, mas não se 
eles devem esperar um comando anterior para rodarem.

\subsubsection{Modelo de Memória}
As threads em execução num kernel tem acesso a 4 locais distintos de memória:

\begin{enumerate}
  \item Memória Global - Toda thread em execução num kernel tem acesso de escrita e leitura a essa região da memória.
  \item Memória Constante - Toda thread em execução num kernel tem acesso de leitura a essa região da memória. Somente o \textit{host} tem acesso de
    escrita a essa parte da memória.
  \item Memória Local - Todas as threads de um work-group tem acesso a essa região da memória. Dependendo do hardware, ela pode ser colocada numa região próxima
    da região de execuçaõ de um work-group ou na memória principal da GPU.
  \item Memória Privada - Região privada de uma thread, somente ela tem acesso a está região.
\end{enumerate}

O \textit{host} tem acesso de escrita e leitura na memória global e constante. O kernel tem acesso de escrita e memória em todas as localidades, a menos da local, 
onde ele só tem acesso de leitura. O OpenCL aplica uma consistência de memória relaxada, ou seja, não existem garantias que o estado de um bloco de memória 
acessado por um work-item seja igual para qualquer outro work-item acessando aquele bloco. A única consistência de memória garantida pelo OpenCL é de que dentro 
de uma barreira de um work-group, tanto a memória global quanto a local será igual para todos os work-itens dentro daquele work-group.

A iteração entre o modelo de memória do \textit{host} e do \textit{device} é feita através de uma API que ou copia dados para a GPU ou faz um mapeamento de um setor
da memória do \textit{host} para um setor da memória do \textit{device}. A passagem da memŕia é feita por uma \textit{Command-Queue}.
A transferência de dados é feita através de um tipo básico de objetos do OpenCL, os 
\textit{Memory Objects}. eles podem ser de 2 tipos:
\begin{itemize}
  \item Tipo \textit{buffer} - Representa tipos primitivos como \textbf{int} ou \textit{float}, vetores e estruturas definidas pelo usuário. Eles são
    acessados pelo kernel através de um ponteiro, e são organizados de maneira sequêncial na memória. Não existe diferença entre o mêtodo de leitura ou escrita
    de um \textit{buffer}.

  \item Tipo \textit{image} - Representa um buffer (não o tipo acima, mas o conceito de buffer na computação) de uma imagem ou de uma textura. Existe uma diferença entre
    os mêtodos de escrita e leitura de um \textit{image}. Para ler ou escrever é necessário usar funções próprias do OpenCL. As funções de leitura transformam o tipo
    \textit{image} num vetor de 4 componentes, e as funções de escrita transformam vetores de 4 componentes em uma componente do tipo \textit{image}.
\end{itemize}

\subsubsection{Modelo de Programação}

Existem 2 models de programação suportados pelo OpenCL:
\begin{enumerate}
  \item Modelo de Dados - Esse é o modelo mais comum usado pelo OpenCL, onde os indices do espaço de indices que cada work-item recebe 
    definem um mapa one-to-one para os dados quie o kernel recebe do \textit{host}. No OpenCL esse modelo é relaxado, já que os
    work-items podem estar associados a mais de um bloco de dados.
  \item Modelo de Tarefas - Esse modelo supõem que somente um work-item será executado em cada device, e que o programador será o responsável
    por paralelizar a aplicação usando ou vários kernels ou tipos vetoriais de dados que o \textit{device} implemente.
\end{enumerate}

Sobre a sincronização entre \textit{device} e \textit{host} no OpenCL, ela pode ser feita de 2 maneiras:
\begin{enumerate}
  \item Pela barreira implicita na execução sequêncial da \textit{command-queue}
  \item Por eventos do OpenCL. Ao rodar um comando numa \textit{command-queue} é possível adicionar um objeto do OpenCL chamado de evento, e podemos esperar
    esse evento ser concluído no \textit{host} para continuar a execução.
\end{enumerate}
