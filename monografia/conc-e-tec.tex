\section{Conceitos e Técnologias}
\subsection{High-Performance Computability}
HPC nasceu da necessidade de poder computacional para resolver uma série de problemas, entre eles:
\begin{itemize}
  \item Previsão climática
  \item Modelação molecular
  \item Simulações físicas
  \item Física quântica
\end{itemize}
Até o final dos anos 90 todos os supercomputadores tinham como base processadores vetoriais. Só no final da década seguinte, 
com o aumento do desempenho das GPUs, que alguns supercomputadores começaram a usar GPUs como suas fontes e processamento.
\subsection{GPU}
A primeira GPU foi a GeForce 256, lançada em 1999. O hardware seguia um pipeline com 2 processos, um que aplicava transformações
em vértices e outro em pixels. Em 2001, a GeForce 3 trouxe o primeiro processador de vértices programável. Em 2005 a primeira
GPU com um processador unificado, usado tanto para pixels como para vértices, foi lançada para o console XBox 360. Para unificar
os 2 processos do pipeline num único processador foi necessário generalizar esse processador, abrindo as portas para programas
paralelos genericos executarem na GPU.

O processamento da placa usada para os testes desse trabalho, a GeForce GTX260 que usa a arquitetura Tesla, segue dois fluxos 
diferentes, um para execução gráfica e outro para paralela. Vamos estudar o fluxo paralelo abaixo.

A placa contém um escalonador implementado em hardware para threads. Ele é responsável por escalonar as threads que serão
executadas nos Thread Processing Clusters (TPCs)A GeForce GTX260 tem 8 TPCs. Cada TPC tem 3 streaming multiprocessors (SM), 
e em cada SM existem 8 streaming-processor (SPs) e um bloco de memória compartilhada. Para os TPCs, a execução segue um padrão
de multiplas intruções, multiplos dados (MIMD), e dentro dos TPCs, nos SMs, a execução segue o padrão única instrução, multiplas
threads (SIMT). Nesse padrão, a mesma instrução é mandada para threads diferentes, e cada thread aplica a instrução em um
conjunto de dados diferentes. O próprio hardware cuida de posiveis separações condicionais no código.

O código que será executado nos SPs é chamado de Kernel. Então, ao executar um kernel na GPU, o hardware criará threads,
cada uma executando esse mesmo kernel mas com dados diferentes, e cada thread será escalonada para um SP diferente. Como
as threads são distribuidas pelos SMs e TPCs varia com a linguagem usada pelo kernel.

Outra parte importante do hardware é a memória. A memória na GPU é limitada em relação à da CPU. GPUs tem, em média, 1GB
de memória, enquanto CPUs tem 4GB. Outro fator é a transferencia de dados da memória principal do computador para a memória 
principal da GPU. A transmissão é feita por um barramento PCI Express, com velocidades de até 16GB/s, dado que o
barramento esteja sendo usado somente para isso. Essa transmissão é a parte mais lenta de todo o
processo de execução na GPU. Em alguns casos é mais viável fazer algumas operações de baixar eficiência numa GPU do que
retornar os dados computados numa GPU para a CPU e passá-los de volta para a GPU para mais operações e retornar esses
dados para a CPU. Com os dados já na memória principal da GPU, eles são copiados para os caches de cada SM por demanda.

É importante conhecer a memória física da placa para qual se está programando por que acesso a memória é um dos maiores
modificadores no desempenho de um programa paralelo. O acesso a memória é concorrente, mas ao utilizar caches e leitura/escritas em
blocos podemos minimizar a taxa com que leituras/escritas conflitantes são feitas. Mas ainda sim é necessário atenção ao escrever um
kernel. Dada a estrutura do hardware da GPU, é melhor deixar threads que façam operações sobre posições de memória próximas no mesmo
SM, assim elas podem utilizar a memória compartilhada do mesmo. que além de ser mais rápido do que buscar os dados na memória principal,
não cria um padrão de buscas frequentes na memoria principal, que acabaria criando uma fila de acesso das threads e diminuiria o desempenho
do programa.
\subsection{CUDA}
Compute Unified Device Architecture (CUDA) é uma arquitetura de programação para GPUs criada pela NVIDIA.
A versão 1.0 foi disponibilizada no inicio de 2007. Atualmente só existe um compilador para CUDA, o nvcc,
e ele só da suporte para GPUs NVIDIA.

O CUDA implementa um conjunto virtual de instruções e memória, tornando os programas retroativos. O compilador
primeiro compila o código em C para um intermediário, chamado de PTX, que depois será convertido em linguagem
de máquina. Na conversão do PTX para linguagem de máquina o compilador verifica quais instruções o hardware
suporte e converte o código para usar as instruções corretas. Novamente, para obter o maior desempenho possível,
é importante saber para qual versão o código final será compilado, pois na passagem do código de uma versão
maior para uma menor não existe a garantia que o algoritmo seguira as mesmas instruções, o compilador pode
mudar um conjunto de instruções para outro menos eficiênte.

A inicialização dos recursos que o CUDA necessita para a comunicação com a GPU é feita no background da
aplicação no momento da primeira chamada de alguma das diretivas do CUDA. Essa primeira diretiva terá um
tempo maior de execução que chamadas subsequentes a mesma diretiva.
\subsubsection{Modelo de Memória}

\subsubsection{Modelo de Execução}
\subsubsection{Modelo de Plataforma}
\subsubsection{Modelo de Programação}
\subsection{OpenCL}
\subsubsection{Modelo de Memória}
\subsubsection{Modelo de Execução}
\subsubsection{Modelo de Plataforma}
\subsubsection{Modelo de Programação}
