\hyphenation{re-gis-tra-do-res}
\hyphenation{re-gis-tra-do-r}
\section{Atividades Realizadas}
\subsection{Comparação das abstrações}
Como as duas linguagens foram desenvolvidas com base num hardware em comum, as suas abstrações são bem parecidas. 
Cada uma delas tem uma abstração para as threads executando o kernel ( \textit{work-item} para o OpenCL e \textit{CUDA threads} para o CUDA). 

Toda thread, em ambas as linguagens, tem um ID único que a identifica em relação a todas as threads em execução (o ID global) 
e um ID que a identifica unicamente dentro de um bloco (o ID local). O ID global é uma combinação do ID local com o ID do bloco. 
É comum usar o ID das threads para identificar quais os dados que ela irá receber. No exemplo desse trabalho, o ID 
global das threads é usado para determinar qual posição das matrizes ela irá usar nas suas operações. 

Para representar a separação das threads nos blocos que serão excalonados para os SM, as duas linguagens implementam uma organização
lógica para separar as threads em blocos (\textit{work-group} no OpenCL e \textit{block} no CUDA).

Os blocos são agrupados em um conjunto maior que engloba todas as threads de um kernel.
No OpenCL, esse conjunto se chama \textit{NDRange} e no CUDA \textit{Grid}. O OpenCL cria um NDRange por execução do kernel
e as dimensões do NDRange e dos work-groups dentro dele são iguais. O espaço de indices das threads de um NDRange pode começar 
tanto de zero quanto de um número definido pelo usuário, facilitanto operações em posições de memória deslocadas dentro 
do espaço de memória do problema.
	
Já no CUDA, os Grids podem ter sua dimensão diferente da dimensão dos blocks. O espaço de indices das threads é limitado a começar 
do zero. A execução de um kernel é representada por um único grid. Notou-se que o compilador do CUDA devolve um erro ao compilar um kernel
que não respeita o tamanho máximo de threads num bloco, enquanto o OpenCL compila, mas o resultado da execução do kernel é sempre
inesperado.

Sobre a memória, as duas linguagens deixam a criação e alocação da memória para o \textit{host}. Cada uma delas define uma maneira
diferente de tratar a memória. No CUDA a memória do device é tratada como um simples ponteiro. Já o OpenCL cria objetos de memória que serão
mapeados para a memória do \textit{device}. As operações de leitura e escrita nesses objetos são feitos através de uma fila de 
execução e de diretivas auxiliares para a inicialização e alocação.

A memória pode ser direcionada para qualquer um dos 4 espaços do device, usando modificadores especiais na declaração da variável
dentro do kernel.
\subsection{Comparação de eficiencia}
\subsubsection{Como fazer a comparação?}
Bem, como fazer a comparação entre essas duas linguagens? A ideia é criar dois tipos de kernels nas duas linguagens, cada tipo
para comparar duas caracteristicas importante das linguagens:
\begin{itemize}
  \item O desempenho ao acessar a memória;
  \item A capacidade de utilizar o processamento da GPU.
\end{itemize}
\subsubsection{Montagem dos kernels}
Para testar o desempenho ao acessar a memória, um kernel que faz a cópia de uma matriz de floats foi usado. O código desse kernel
tanto em OpenCL:

\begin{lstlisting}
  __kernel void MatrixCopy (__global float* a, 
                            __global float* b, 
                            __global int* rowSize, 
                            __global int* columnSize) {
    unsigned int row = get_global_id(0);
    unsigned int column = get_global_id(1);
	  b[row+column*(*rowSize)] = a[row+column*(*rowSize)];
  }
\end{lstlisting}

Como em CUDA:

\begin{lstlisting}
  __global__ void MatrixCopy (float* MatrixA, 
                              float* MatrixB, 
                              int rowSize, 
                              int columnSize) {
    int row = blockIdx.x*blockDim.x+threadIdx.x;
    int column = blockIdx.y*blockDim.y+threadIdx.y;
    MatrixB[row+column*columnSize] = MatrixA[row+column*columnSize];
  }
\end{lstlisting}

As primeiras linhas de cada kernel determinam qual posição da matriz será copiada usando o ID global da thread.
A última linha faz a cópia da matriz A para a matriz B. \\

Já para testar a capacidade do processamento das linguagens usamos um kernel que faz a multiplicação de duas matrizes de floats e guarda
o resultado numa terceira.
Em OpenCL:
\begin{lstlisting}
  __kernel void matrixmulti(__global float* MatrixA, 
                            __global float* MatrixB, 
                            __global float* MatrixC, 
                            __global int* N) {
    unsigned i = get_global_id(0);
    unsigned j = get_global_id(1);
    unsigned k;
    MatrixC[i*(*N)+j] = 0;
    for( k = 0; k < (*N); k++ ) 
    	MatrixC[i*(*N)+j] += MatrixA[i*(*N)+k]*MatrixB[j+k*(*N)];
  }
\end{lstlisting}
E em CUDA:
\begin{lstlisting}
  __global__ void MatrixCopy (float* MatrixA, 
                              float* MatrixB, 
                              float* MatrixC, 
                              int N) {
    int row = blockIdx.x*blockDim.x+threadIdx.x;
    int column = blockIdx.y*blockDim.y+threadIdx.y;
    int k;
    MatrixC[column*N+row] = 0;
    for (k = 0; k < N; k++ )
      MatrixC[column*N+row] += MatrixA[column*N+k]*MatrixB[k*N+row];
  }
\end{lstlisting}

Novamente, as primeiras linha fazem a distribuição da posição de memória para cada thread, enquanto as duas últimas linhas fazem a multiplicação em si.

\subsection{Os arquivos .ptx}

Ao executar um kernel numa GPU NVIDIA ele não é executado diretamente no hardware, na verdade ele passa pela máquina virtual PTX, como dito anteriormente.
Ao descobrir esse fato, decidimos comparar os .ptx resultantes da compilação dos nossos kernels para a máquina PTX.
Os arquivos ptx usam uma linguagem parecida com o \textbf{Assembly}, com comandos especiais para as operações únicas de uma GPU, como operações vetoriais.

Para gerar o PTX de um kernel basta acresentar a flag de compilação \textit{--ptx} para o \textit{nvcc}. \\

Os comandos PTX podem conter modificadores, por exemplo:

\begin{lstlisting}
  ld.param.u64 	%rl1, [_Z10MatrixCopyPfS_ii_param_0];
\end{lstlisting}

em que o comando \textit{ld}, usado para preencher algum endereço de memória, usa o modificador \textit{.param} para carregar um parâmetro do kernel,
enquanto no exemplo abaixo,

\begin{lstlisting}
  ld.global.f32 	%f1, [%rl6];
\end{lstlisting}

o mesmo comando usando em conjunto com o modificador \textit{.global} irá buscar na memória global o que deve ser carregado para o registrador.
Os modificadores \textit{.u64} e \textit{.f32} definem o tipo a ser tratado, no caso \textit{.u} é um \textbf{Unsigned Int} e \textit{.f} um
\textbf{Float}, e o número define o tamanho do endereço. 

Nas GPUs NVIDIA, onde várias threads compartilham o mesmo ponteiro de instrução, cada thread pode modificar os seus registradores para que eles 
funcionem como registradores booleanos usando a diretiva \textit{.reg .pred}. Com isso, esses registradores podem receber valores de operadores
lógicos definidos dentro do PTX. Eles podem ser usandos em conjunto com o símbolo @, que no PTX define se uma instrução será ou não executada
baseando-se no valor do operador lógico adjunto a ela.
Por exemplo:

\begin{lstlisting}
  @%p1 bra 	BB0_3;
\end{lstlisting} 

A instrução \textit{bra} só será executada se o registrador \textit{p1} conter \textbf{TRUE}. \\

A instrução \textit{bra} define uma separação na estrutura de execução de um warp. Ao encontrar um branch, todas as threads que seguirem esse branch 
ganham um novo apontador de instruções e continuam a sua execução em paralelo, criando um novo segmento de execução. Por exemplo, se de 16 threads 
5 entram em um \textbf{if} e o restante passa por ele, temos 2 segmentos de execução. Todas as threads de um mesmo segmento são executadas concorrentemente, 
enquanto as threads de outro segmento esperam sua vez para executar. Então no nosso exemplo, no primeiro ciclo dos processadores de um SM 5 threads serão
executadas, e no próximo ciclo 11 threads, e no seguinte as 5 iniciais, e assim por diante.
 
Agora que já cobrimos as peculiaridades importantes do PTX, vamos usar como exemplo para estudar a execução de um PTX o kernel de cópia de memória feito
em CUDA.

\begin{lstlisting}
  .version 3.0
  .target sm_20
  .address_size 64

  .file	1 "/tmp/tmpxft_00001b52_00000000-9_memory.cpp3.i"
  .file	2 "memory.cu"

  .entry _Z10MatrixCopyPfS_ii(
	  .param .u64 _Z10MatrixCopyPfS_ii_param_0,
	  .param .u64 _Z10MatrixCopyPfS_ii_param_1,
	  .param .u32 _Z10MatrixCopyPfS_ii_param_2,
	  .param .u32 _Z10MatrixCopyPfS_ii_param_3
  )
  {
	  .reg .f32 	%f<2>;
	  .reg .s32 	%r<13>;
	  .reg .s64 	%rl<8>;


	  ld.param.u64 	%rl1, [_Z10MatrixCopyPfS_ii_param_0];
	  ld.param.u64 	%rl2, [_Z10MatrixCopyPfS_ii_param_1];
	  ld.param.u32 	%r1, [_Z10MatrixCopyPfS_ii_param_3];
	  cvta.to.global.u64 	%rl3, %rl2;
	  mov.u32 	%r2, %ntid.x;
	  mov.u32 	%r3, %ctaid.x;
	  mov.u32 	%r4, %tid.x;
	  mad.lo.s32 	%r5, %r2, %r3, %r4;
	  mov.u32 	%r6, %ntid.y;
	  mov.u32 	%r7, %ctaid.y;
	  mov.u32 	%r8, %tid.y;
	  mad.lo.s32 	%r9, %r6, %r7, %r8;
	  mad.lo.s32 	%r10, %r5, %r1, %r9;
	  cvta.to.global.u64 	%rl4, %rl1;
	  mul.wide.s32 	%rl5, %r10, 4;
	  add.s64 	%rl6, %rl4, %rl5;
	  add.s64 	%rl7, %rl3, %rl5;
	  ld.global.f32 	%f1, [%rl6];
	  st.global.f32 	[%rl7], %f1;
	  ret;
  }
\end{lstlisting}

As primeiras linhas,

\begin{lstlisting}
  .version 3.0
  .target sm_20
  .address_size 64
\end{lstlisting}

definem o ambiente que deve ser preparado na GPU para a execução do kernel. A primeira linha define a versão da máquina PTX, a segunda
qual a versão da API de comunicação com a GPU deve ser usada e a última o tamanho do endereçamento a ser usado. \\
As próximas linhas, 

\begin{lstlisting}
  .file	1 "/tmp/tmpxft_00001b52_00000000-9_memory.cpp3.i"
  .file	2 "memory.cu"
\end{lstlisting}

associam um inteiro aos arquivos que podem ser usados no kernel. Esses arquivos são acessados usando esse índice, caso necessário.
Nesse caso o primeiro \textit{.file} associa a 1 o binário do kernel e a 2 o código fonte.\\

Agora, ao kernel em si. A próxima linha,

\begin{lstlisting}
  .entry _Z10MatrixCopyPfS_ii(
	  .param .u64 _Z10MatrixCopyPfS_ii_param_0,
	  .param .u64 _Z10MatrixCopyPfS_ii_param_1,
	  .param .u32 _Z10MatrixCopyPfS_ii_param_2,
	  .param .u32 _Z10MatrixCopyPfS_ii_param_3
  )
\end{lstlisting}

define tanto o ponto de entrada da execução quanto os parâmetros recebidos pelo kernel. A diretiva \textit{.entry} define o ponto de inicio da execução do kernel.
Os \textit{.param} definem um meio do kernel acessar os parâmetros passados pelo \textit{Host}, além de configurar o tamanho do endereçamento deles. 
O último parâmetro da diretiva \textit{.param} é a tag que será usada pelo comando \textit{ld.param} para carregar os parâmetros em registradores. \\

Já em execução, a primeira coisa que uma thread faz é alocar os registradores que ela irá usar, com a diretiva \textit{.reg},

\begin{lstlisting}
	  .reg .f32 	%f<2>;
	  .reg .s32 	%r<13>;
	  .reg .s64 	%rl<8>;
\end{lstlisting}

Os parâmetros dessa diretiva definem o tipo do registrador ( \textit{\%f} para \textbf{Float} ) e o número de registradores ( \textit{<n>} para $n$ registradores ). \\

A próxima etapa carrega os parâmetros em registradores,

\begin{lstlisting}
	  ld.param.u64 	%rl1, [_Z10MatrixCopyPfS_ii_param_0];
	  ld.param.u64 	%rl2, [_Z10MatrixCopyPfS_ii_param_1];
	  ld.param.u32 	%r1, [_Z10MatrixCopyPfS_ii_param_3];
\end{lstlisting}

É importante lembrar que os dois primeiros parâmetros são ponteiros. Ao carregar um ponteiro num registrador, 
o PTX não sabe se esse endereço faz referência a memória local, global, constante ou a dividida entre as threads, 
então a próxima instrução é usada para transformar um ponteiro genérico em um ponteiro global. É possível determinar
a qual posição da memória o ponteiro aponta ao definir os parâmetros, mas o CUDA não faz isso.

\begin{lstlisting}
  cvta.to.global.u64 	%rl3, %rl2;
\end{lstlisting}

Com os parâmetros necessários carregados e devidamente ajustados, o pró-\newline ximo passo do kernel é calcular o índice da thread. A GPU tem 3 registradores
específicos que guardam o índice local de uma thread. As próximas instruções mostram como calculcar o índice global de uma thread num kernel de 
2 dimensões:

\begin{lstlisting}
    mov.u32 	%r2, %ntid.x;
    mov.u32 	%r3, %ctaid.x;
    mov.u32 	%r4, %tid.x;
    mad.lo.s32 	%r5, %r2, %r3, %r4;
    mov.u32 	%r6, %ntid.y;
    mov.u32 	%r7, %ctaid.y;
    mov.u32 	%r8, %tid.y;
    mad.lo.s32 	%r9, %r6, %r7, %r8;
    mad.lo.s32 	%r10, %r5, %r1, %r9;
\end{lstlisting}

A instrução \textit{mov} preenche um registrador com dados de uma posição não-genérica de memória. A instrução \textit{mad} multiplica o segundo argumento
pelo terceiro, soma o quarto à multiplicação e guarda o resultado total em um registrador. Os dois primeiros \textit{mad} calculam os índices da thread,
cada um numa dimensão diferente. O terceiro \textit{mad} usa os dois índices e o parâmetro que contém o tamanho da matriz para calcular em qual posição da 
matriz a thread atual irá operar. \\

O que resta é copiar os dados de uma matriz para a outra.

\begin{lstlisting}
	cvta.to.global.u64 	%rl4, %rl1;
  mul.wide.s32 	%rl5, %r10, 4;
	add.s64 	%rl6, %rl4, %rl5;
	add.s64 	%rl7, %rl3, %rl5;
	ld.global.f32 	%f1, [%rl6];
	st.global.f32 	[%rl7], %f1;
  ret;
\end{lstlisting}

O último parâmetro usado, o ponteiro para a matriz que será copiada, é transformado pela instrução \textit{cvta}. Os dois \textit{add} adicionam os índices
da thread ao ponteiro das matrizes, criando um offset que referência a posição que aquela thread deve usar para a cópia. O \textit{ld} carrega o valor dessa
posição num registrador que depois é copiado para a matriz destino usando a instrução \textit{st}, e por fim o kernel finaliza. Não foi encontrado nada
na documentação do PTX nem nenhum motivo aparente no kernel que explique a multiplicação da posição da matriz que será operada por quatro, mas essa operação
é constante nos dois tipos de kernel e tanto no OpenCL quanto no CUDA.
